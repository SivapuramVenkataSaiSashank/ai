{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13185609,"sourceType":"datasetVersion","datasetId":8355777},{"sourceId":13194356,"sourceType":"datasetVersion","datasetId":8361492}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"f3ddd469-65fe-41a1-a428-f5ac60906e34","cell_type":"code","source":"!pip install /kaggle/input/pymupdf-wheels/pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T08:38:01.462789Z","iopub.execute_input":"2025-09-28T08:38:01.463077Z","iopub.status.idle":"2025-09-28T08:38:07.615966Z","shell.execute_reply.started":"2025-09-28T08:38:01.463057Z","shell.execute_reply":"2025-09-28T08:38:07.615236Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/pymupdf-wheels/pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl\nInstalling collected packages: pymupdf\nSuccessfully installed pymupdf-1.26.4\n","output_type":"stream"}],"execution_count":5},{"id":"a0cdcbe1-acc1-4156-89ca-83ce5dfa9e62","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"85ea190e-85de-4409-91ba-52622cd74941","cell_type":"code","source":"!pip install pillow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T08:38:09.011725Z","iopub.execute_input":"2025-09-28T08:38:09.012500Z","iopub.status.idle":"2025-09-28T08:38:12.025199Z","shell.execute_reply.started":"2025-09-28T08:38:09.012472Z","shell.execute_reply":"2025-09-28T08:38:12.024447Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n","output_type":"stream"}],"execution_count":6},{"id":"a753facd-1e41-4d50-8a09-ecab44615d1d","cell_type":"code","source":"import os\nimport fitz  # PyMuPDF\n\ndef convert_pdf_folder_to_images(pdf_folder, out_folder, img_size=(256,256)):\n    os.makedirs(out_folder, exist_ok=True)\n    for pdf_file in os.listdir(pdf_folder):\n        if pdf_file.lower().endswith('.pdf'):\n            pdf_path = os.path.join(pdf_folder, pdf_file)\n            try:\n                doc = fitz.open(pdf_path)\n                for i, page in enumerate(doc):\n                    zoom = 200/72  # render quality\n                    mat = fitz.Matrix(zoom, zoom)\n                    pix = page.get_pixmap(matrix=mat)\n                    \n                    img_path = os.path.join(out_folder, f\"{pdf_file[:-4]}_{i}.png\")\n                    pix.save(img_path)\n                doc.close()\n                print(f\"✅ Converted: {pdf_file}\")\n            except Exception as e:\n                print(\"❌ Failed to convert:\", pdf_file, \"|\", e)\n\n# Example usage\noriginal_folders = [\n    \"/kaggle/input/ai-trace-finder/data/Originals/official\",\n    \"/kaggle/input/ai-trace-finder/data/Originals/wikipedia\"\n]\n\nfor folder in original_folders:\n    out_folder = os.path.join(\"/kaggle/working/Original\", os.path.basename(folder))\n    convert_pdf_folder_to_images(folder, out_folder)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T08:38:13.456790Z","iopub.execute_input":"2025-09-28T08:38:13.457568Z","iopub.status.idle":"2025-09-28T08:38:42.291659Z","shell.execute_reply.started":"2025-09-28T08:38:13.457536Z","shell.execute_reply":"2025-09-28T08:38:42.291098Z"}},"outputs":[{"name":"stdout","text":"✅ Converted: 75.pdf\n✅ Converted: 23.pdf\n✅ Converted: 73.pdf\n✅ Converted: 41.pdf\n✅ Converted: 25.pdf\n✅ Converted: 31.pdf\n✅ Converted: 45.pdf\n✅ Converted: 98.pdf\n✅ Converted: 30.pdf\n✅ Converted: 49.pdf\n✅ Converted: 24.pdf\n✅ Converted: 63.pdf\n✅ Converted: 36.pdf\n✅ Converted: 15.pdf\n✅ Converted: 89.pdf\n✅ Converted: 5.pdf\n✅ Converted: 93.pdf\n✅ Converted: 56.pdf\n✅ Converted: 29.pdf\n✅ Converted: 8.pdf\n✅ Converted: 86.pdf\n✅ Converted: 14.pdf\n✅ Converted: 17.pdf\n✅ Converted: 27.pdf\n✅ Converted: 77.pdf\n✅ Converted: 20.pdf\n✅ Converted: 81.pdf\n✅ Converted: 58.pdf\n✅ Converted: 55.pdf\n✅ Converted: 39.pdf\n✅ Converted: 26.pdf\n✅ Converted: 79.pdf\n✅ Converted: 64.pdf\n✅ Converted: 95.pdf\n✅ Converted: 91.pdf\n✅ Converted: 38.pdf\n✅ Converted: 12.pdf\n✅ Converted: 9.pdf\n✅ Converted: 7.pdf\n✅ Converted: 47.pdf\n✅ Converted: 40.pdf\n✅ Converted: 66.pdf\n✅ Converted: 4.pdf\n✅ Converted: 74.pdf\n✅ Converted: 78.pdf\n✅ Converted: 54.pdf\n✅ Converted: 44.pdf\n✅ Converted: 19.pdf\n✅ Converted: 100.pdf\n✅ Converted: 6.pdf\n✅ Converted: 53.pdf\n✅ Converted: 99.pdf\n✅ Converted: 94.pdf\n✅ Converted: 97.pdf\n✅ Converted: 61.pdf\n✅ Converted: 34.pdf\n✅ Converted: 1.pdf\n✅ Converted: 42.pdf\n✅ Converted: 96.pdf\n✅ Converted: 88.pdf\n✅ Converted: 80.pdf\n✅ Converted: 10.pdf\n✅ Converted: 60.pdf\n✅ Converted: 22.pdf\n✅ Converted: 21.pdf\n✅ Converted: 48.pdf\n✅ Converted: 51.pdf\n✅ Converted: 59.pdf\n✅ Converted: 32.pdf\n✅ Converted: 28.pdf\n✅ Converted: 68.pdf\n✅ Converted: 65.pdf\n✅ Converted: 33.pdf\n✅ Converted: 87.pdf\n✅ Converted: 76.pdf\n✅ Converted: 72.pdf\n✅ Converted: 85.pdf\n✅ Converted: 57.pdf\n✅ Converted: 13.pdf\n✅ Converted: 46.pdf\n✅ Converted: 69.pdf\n✅ Converted: 84.pdf\n✅ Converted: 67.pdf\n✅ Converted: 16.pdf\n✅ Converted: 82.pdf\n✅ Converted: 35.pdf\n✅ Converted: 11.pdf\n✅ Converted: 70.pdf\n✅ Converted: 83.pdf\n✅ Converted: 50.pdf\n✅ Converted: 71.pdf\n✅ Converted: 52.pdf\n✅ Converted: 92.pdf\n✅ Converted: 3.pdf\n✅ Converted: 2.pdf\n✅ Converted: 90.pdf\n✅ Converted: 43.pdf\n✅ Converted: 37.pdf\n✅ Converted: 62.pdf\n✅ Converted: 18.pdf\n✅ Converted: 75.pdf\n✅ Converted: 102.pdf\n✅ Converted: 23.pdf\n✅ Converted: 73.pdf\n✅ Converted: 41.pdf\n✅ Converted: 25.pdf\n✅ Converted: 31.pdf\n✅ Converted: 45.pdf\n✅ Converted: 98.pdf\n✅ Converted: 30.pdf\n✅ Converted: 49.pdf\n✅ Converted: 24.pdf\n✅ Converted: 63.pdf\n✅ Converted: 36.pdf\n✅ Converted: 15.pdf\n✅ Converted: 89.pdf\n✅ Converted: 5.pdf\n✅ Converted: 93.pdf\n✅ Converted: 56.pdf\n✅ Converted: 29.pdf\n✅ Converted: 8.pdf\n✅ Converted: 86.pdf\n✅ Converted: 103.pdf\n✅ Converted: 14.pdf\n✅ Converted: 17.pdf\n✅ Converted: 27.pdf\n✅ Converted: 77.pdf\n✅ Converted: 20.pdf\n✅ Converted: 81.pdf\n✅ Converted: 58.pdf\n✅ Converted: 55.pdf\n✅ Converted: 39.pdf\n✅ Converted: 26.pdf\n✅ Converted: 79.pdf\n✅ Converted: 64.pdf\n✅ Converted: 95.pdf\n✅ Converted: 91.pdf\n✅ Converted: 38.pdf\n✅ Converted: 12.pdf\n✅ Converted: 9.pdf\n✅ Converted: 7.pdf\n✅ Converted: 47.pdf\n✅ Converted: 40.pdf\n✅ Converted: 66.pdf\n✅ Converted: 4.pdf\n✅ Converted: 74.pdf\n✅ Converted: 78.pdf\n✅ Converted: 54.pdf\n✅ Converted: 44.pdf\n✅ Converted: 108.pdf\n✅ Converted: 19.pdf\n✅ Converted: 100.pdf\n✅ Converted: 6.pdf\n✅ Converted: 53.pdf\n✅ Converted: 99.pdf\n✅ Converted: 94.pdf\n✅ Converted: 97.pdf\n✅ Converted: 61.pdf\n✅ Converted: 34.pdf\n✅ Converted: 1.pdf\n✅ Converted: 42.pdf\n✅ Converted: 96.pdf\n✅ Converted: 88.pdf\n✅ Converted: 80.pdf\n✅ Converted: 10.pdf\n✅ Converted: 105.pdf\n✅ Converted: 60.pdf\n✅ Converted: 22.pdf\n✅ Converted: 21.pdf\n✅ Converted: 48.pdf\n✅ Converted: 51.pdf\n✅ Converted: 59.pdf\n✅ Converted: 32.pdf\n✅ Converted: 28.pdf\n✅ Converted: 68.pdf\n✅ Converted: 65.pdf\n✅ Converted: 33.pdf\n✅ Converted: 87.pdf\n✅ Converted: 76.pdf\n✅ Converted: 72.pdf\n✅ Converted: 85.pdf\n✅ Converted: 57.pdf\n✅ Converted: 13.pdf\n✅ Converted: 46.pdf\n✅ Converted: 101.pdf\n✅ Converted: 106.pdf\n✅ Converted: 69.pdf\n✅ Converted: 84.pdf\n✅ Converted: 67.pdf\n✅ Converted: 16.pdf\n✅ Converted: 82.pdf\n✅ Converted: 107.pdf\n✅ Converted: 35.pdf\n✅ Converted: 11.pdf\n✅ Converted: 70.pdf\n✅ Converted: 83.pdf\n✅ Converted: 50.pdf\n✅ Converted: 71.pdf\n✅ Converted: 52.pdf\n✅ Converted: 92.pdf\n✅ Converted: 3.pdf\n✅ Converted: 2.pdf\n✅ Converted: 104.pdf\n✅ Converted: 90.pdf\n✅ Converted: 43.pdf\n✅ Converted: 37.pdf\n✅ Converted: 62.pdf\n✅ Converted: 18.pdf\n","output_type":"stream"}],"execution_count":7},{"id":"d00f4219-394f-4a26-83a3-1c8e3ea412fe","cell_type":"code","source":"import fitz  # PyMuPDF for PDFs\nfrom PIL import Image, ImageSequence\nimport os\n\ndef convert_tampered_to_images(tampered_folder, out_folder, img_size=(256, 256)):\n    \"\"\"\n    Converts all PDFs, TIFFs, and image files in tampered_folder (including subfolders)\n    into RGB PNG images resized to img_size. Preserves folder structure.\n    \"\"\"\n    for root, dirs, files in os.walk(tampered_folder):\n        # Preserve folder structure in the output\n        relative_path = os.path.relpath(root, tampered_folder)\n        target_dir = os.path.join(out_folder, relative_path)\n        os.makedirs(target_dir, exist_ok=True)\n\n        for fname in files:\n            fpath = os.path.join(root, fname)\n            ext = fname.lower().split(\".\")[-1]\n\n            try:\n                # ---- Case 1: PDF using fitz ----\n                if ext == \"pdf\":\n                    doc = fitz.open(fpath)\n                    for i, page in enumerate(doc):\n                        pix = page.get_pixmap()\n                        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n                        img = img.resize(img_size)\n                        img.save(os.path.join(target_dir, f\"{fname[:-4]}_{i}.png\"))\n\n                # ---- Case 2: TIF / TIFF ----\n                elif ext in [\"tif\", \"tiff\"]:\n                    img = Image.open(fpath)\n                    for i, page in enumerate(ImageSequence.Iterator(img)):\n                        page = page.convert(\"RGB\").resize(img_size)\n                        page.save(os.path.join(target_dir, f\"{fname[:-4]}_{i}.png\"))\n\n                # ---- Case 3: PNG / JPG / JPEG ----\n                elif ext in [\"png\", \"jpg\", \"jpeg\"]:\n                    img = Image.open(fpath).convert(\"RGB\").resize(img_size)\n                    img.save(os.path.join(target_dir, fname[:-4] + \".png\"))\n\n                else:\n                    print(\"Skipping unsupported file:\", fname)\n\n            except Exception as e:\n                print(\"Failed to convert:\", fname, \"|\", e)\n\n# Example usage\ntampered_folder = \"/kaggle/input/ai-trace-finder/data/tampered\"\nout_tampered = \"/kaggle/working/Tampered\"\n\nconvert_tampered_to_images(tampered_folder, out_tampered)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T08:40:37.145393Z","iopub.execute_input":"2025-09-28T08:40:37.146128Z","iopub.status.idle":"2025-09-28T08:41:49.570548Z","shell.execute_reply.started":"2025-09-28T08:40:37.146099Z","shell.execute_reply":"2025-09-28T08:41:49.569997Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"}],"execution_count":8},{"id":"45f05fc2-9448-41aa-a46c-489646dddbe3","cell_type":"code","source":"from torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Image transformations (resize, normalize, etc.)\nimg_size = 256\ntransform = transforms.Compose([\n    transforms.Resize((img_size, img_size)),\n    transforms.ToTensor(),\n])\n\n# ---- Original Dataset ----\norig_dataset = datasets.ImageFolder(root=\"/kaggle/working/Original\", transform=transform)\norig_loader = DataLoader(orig_dataset, batch_size=32, shuffle=True)\n\n# ---- Tampered Dataset ----\ntampered_dataset = datasets.ImageFolder(root=\"/kaggle/working/Tampered\", transform=transform)\ntampered_loader = DataLoader(tampered_dataset, batch_size=32, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T08:42:31.148078Z","iopub.execute_input":"2025-09-28T08:42:31.148344Z","iopub.status.idle":"2025-09-28T08:42:31.157245Z","shell.execute_reply.started":"2025-09-28T08:42:31.148325Z","shell.execute_reply":"2025-09-28T08:42:31.156487Z"}},"outputs":[],"execution_count":10},{"id":"14c27645-7be6-4b55-9158-cdde02e42f5f","cell_type":"code","source":"import os\nimport glob\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T08:42:35.230305Z","iopub.execute_input":"2025-09-28T08:42:35.231057Z","iopub.status.idle":"2025-09-28T08:42:35.236059Z","shell.execute_reply.started":"2025-09-28T08:42:35.231031Z","shell.execute_reply":"2025-09-28T08:42:35.235509Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":11},{"id":"970fb39b-73fa-4a60-8e37-0396ad1a940a","cell_type":"code","source":"class ImageDataset(Dataset):\n    def __init__(self, paths, labels, transform=None, img_size=(256,256)):\n        self.paths = paths\n        self.labels = labels\n        self.transform = transform\n        self.img_size = img_size\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, idx):\n        img = Image.open(self.paths[idx]).convert(\"RGB\").resize(self.img_size)\n        if self.transform:\n            img = self.transform(img)\n        label = self.labels[idx]\n        return img, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T08:42:38.285143Z","iopub.execute_input":"2025-09-28T08:42:38.285418Z","iopub.status.idle":"2025-09-28T08:42:38.290288Z","shell.execute_reply.started":"2025-09-28T08:42:38.285397Z","shell.execute_reply":"2025-09-28T08:42:38.289748Z"}},"outputs":[],"execution_count":12},{"id":"cb4fb4fd-58ab-4e3d-819f-6ff73701fbe3","cell_type":"code","source":"# Folders\norig_folder = \"/kaggle/working/Original\"\ntampered_folder = \"/kaggle/working/Tampered\"\n\n# Get subfolders and map to labels\nall_folders = sorted(os.listdir(orig_folder)) + sorted(os.listdir(tampered_folder))\nlabel_map = {name: i for i, name in enumerate(all_folders)}\n\n# Collect image paths and labels\nall_paths = []\nall_labels = []\n\nfor folder in sorted(os.listdir(orig_folder)):\n    folder_path = os.path.join(orig_folder, folder)\n    for f in os.listdir(folder_path):\n        all_paths.append(os.path.join(folder_path, f))\n        all_labels.append(label_map[folder])\n\nfor folder in sorted(os.listdir(tampered_folder)):\n    folder_path = os.path.join(tampered_folder, folder)\n    for f in os.listdir(folder_path):\n        all_paths.append(os.path.join(folder_path, f))\n        all_labels.append(label_map[folder])\n\n# Train/test split\ntrain_paths, test_paths, train_labels, test_labels = train_test_split(\n    all_paths, all_labels, test_size=0.2, random_state=42, stratify=all_labels\n)\n\n# Transform for CNN\ncnn_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n])\n\ntrain_dataset = ImageDataset(train_paths, train_labels, transform=cnn_transform)\ntest_dataset = ImageDataset(test_paths, test_labels, transform=cnn_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\nprint(\"Number of classes:\", len(label_map))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T08:42:40.840241Z","iopub.execute_input":"2025-09-28T08:42:40.840932Z","iopub.status.idle":"2025-09-28T08:42:40.857330Z","shell.execute_reply.started":"2025-09-28T08:42:40.840904Z","shell.execute_reply":"2025-09-28T08:42:40.856771Z"}},"outputs":[{"name":"stdout","text":"Number of classes: 5\n","output_type":"stream"}],"execution_count":13},{"id":"e7e948d2-2626-42c6-81e5-db55705b47ae","cell_type":"code","source":"def train_cnn(model, dataloader, criterion, optimizer, epochs=5):\n    model.train()\n    for epoch in range(epochs):\n        running_loss = 0\n        for imgs, labels in dataloader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(imgs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(dataloader):.4f}\")\n\ndef evaluate_cnn(model, dataloader):\n    model.eval()\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for imgs, labels in dataloader:\n            imgs = imgs.to(device)\n            outputs = model(imgs)\n            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n            all_preds.extend(preds)\n            all_labels.extend(labels.numpy())\n    acc = accuracy_score(all_labels, all_preds)\n    print(\"CNN Accuracy:\", acc)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T08:42:44.279536Z","iopub.execute_input":"2025-09-28T08:42:44.279921Z","iopub.status.idle":"2025-09-28T08:42:44.285972Z","shell.execute_reply.started":"2025-09-28T08:42:44.279897Z","shell.execute_reply":"2025-09-28T08:42:44.285284Z"}},"outputs":[],"execution_count":14},{"id":"886d3c07-9fc4-4823-8daa-2a2d40c15274","cell_type":"code","source":"from torch.utils.data import Dataset\nfrom PIL import Image\nimport os\n\nclass ImagePathDataset(Dataset):\n    def __init__(self, root_folders, label_map, transform=None, img_size=(256,256)):\n        \"\"\"\n        root_folders: list of top-level folders, e.g., [\"/kaggle/working/Original\", \"/kaggle/working/Tampered\"]\n        label_map: dict mapping top-level folder names to labels, e.g., {\"Original\":0, \"Tampered\":1}\n        \"\"\"\n        self.paths = []\n        self.labels = []\n        self.transform = transform\n        self.img_size = img_size\n\n        def is_image_file(f):\n            return f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))\n\n        for root_folder in root_folders:\n            top_label = label_map[os.path.basename(root_folder)]\n            # Walk recursively\n            for dirpath, dirnames, filenames in os.walk(root_folder):\n                for f in filenames:\n                    if is_image_file(f):\n                        self.paths.append(os.path.join(dirpath, f))\n                        self.labels.append(top_label)\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, idx):\n        img_path = self.paths[idx]\n        img = Image.open(img_path).convert(\"RGB\").resize(self.img_size)\n        if self.transform:\n            img = self.transform(img)\n        label = self.labels[idx]\n        return img, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T08:46:15.250433Z","iopub.execute_input":"2025-09-28T08:46:15.250998Z","iopub.status.idle":"2025-09-28T08:46:15.257569Z","shell.execute_reply.started":"2025-09-28T08:46:15.250974Z","shell.execute_reply":"2025-09-28T08:46:15.256881Z"}},"outputs":[],"execution_count":20},{"id":"979289f6-6fbc-44ac-b566-1a1efa8d2fb0","cell_type":"code","source":"label_map = {\"Original\": 0, \"Tampered\": 1}\n\ndataset = ImagePathDataset(\n    root_folders=[\"/kaggle/working/Original\", \"/kaggle/working/Tampered\"],\n    label_map=label_map,\n    transform=cnn_transform,\n    img_size=(256,256)\n)\n\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, Subset\n\ntrain_idx, test_idx = train_test_split(list(range(len(dataset))), test_size=0.2, stratify=dataset.labels, random_state=42)\n\ntrain_dataset = Subset(dataset, train_idx)\ntest_dataset = Subset(dataset, test_idx)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T08:46:40.860474Z","iopub.execute_input":"2025-09-28T08:46:40.861192Z","iopub.status.idle":"2025-09-28T08:46:40.870627Z","shell.execute_reply.started":"2025-09-28T08:46:40.861159Z","shell.execute_reply":"2025-09-28T08:46:40.869935Z"}},"outputs":[],"execution_count":22},{"id":"889901cb-5c7e-49e0-ae0f-d315510b4656","cell_type":"code","source":"# ResNet18\nresnet18_model = models.resnet18(weights=None)\nresnet18_model.fc = nn.Linear(resnet18_model.fc.in_features, len(label_map))\nresnet18_model = resnet18_model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(resnet18_model.parameters(), lr=1e-4)\n\nprint(\"Training ResNet18...\")\ntrain_cnn(resnet18_model, train_loader, criterion, optimizer, epochs=5)\n\nprint(\"Evaluating ResNet18...\")\nevaluate_cnn(resnet18_model, test_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T08:46:42.874453Z","iopub.execute_input":"2025-09-28T08:46:42.874977Z","iopub.status.idle":"2025-09-28T08:47:46.365213Z","shell.execute_reply.started":"2025-09-28T08:46:42.874951Z","shell.execute_reply":"2025-09-28T08:47:46.364498Z"}},"outputs":[{"name":"stdout","text":"Training ResNet18...\nEpoch 1, Loss: 0.4320\nEpoch 2, Loss: 0.1778\nEpoch 3, Loss: 0.1035\nEpoch 4, Loss: 0.0308\nEpoch 5, Loss: 0.0246\nEvaluating ResNet18...\nCNN Accuracy: 0.8446601941747572\n","output_type":"stream"}],"execution_count":23},{"id":"663c1a11-e06f-4364-a18d-cd11c1303267","cell_type":"code","source":"def extract_features(model, dataloader):\n    model.eval()\n    features = []\n    labels = []\n    with torch.no_grad():\n        for imgs, lbls in dataloader:\n            imgs = imgs.to(device)\n            x = model.conv1(imgs)\n            x = model.bn1(x)\n            x = model.relu(x)\n            x = model.maxpool(x)\n            x = model.layer1(x)\n            x = model.layer2(x)\n            x = model.layer3(x)\n            x = model.layer4(x)\n            x = model.avgpool(x)\n            x = torch.flatten(x, 1).cpu().numpy()\n            features.extend(x)\n            labels.extend(lbls.numpy())\n    return features, labels\n\ntrain_feats, train_lbls = extract_features(resnet18_model, train_loader)\ntest_feats, test_lbls = extract_features(resnet18_model, test_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T08:48:28.260170Z","iopub.execute_input":"2025-09-28T08:48:28.260859Z","iopub.status.idle":"2025-09-28T08:48:41.567117Z","shell.execute_reply.started":"2025-09-28T08:48:28.260834Z","shell.execute_reply":"2025-09-28T08:48:41.566512Z"}},"outputs":[],"execution_count":24},{"id":"1027ae9d-8125-4b6e-9875-2b39e419c1a2","cell_type":"code","source":"# SVM\nsvm_model = SVC(kernel='linear')\nsvm_model.fit(train_feats, train_lbls)\nsvm_preds = svm_model.predict(test_feats)\nprint(\"SVM Accuracy:\", accuracy_score(test_lbls, svm_preds))\n\n# Random Forest\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(train_feats, train_lbls)\nrf_preds = rf_model.predict(test_feats)\nprint(\"Random Forest Accuracy:\", accuracy_score(test_lbls, rf_preds))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T08:48:45.640099Z","iopub.execute_input":"2025-09-28T08:48:45.640362Z","iopub.status.idle":"2025-09-28T08:48:45.871880Z","shell.execute_reply.started":"2025-09-28T08:48:45.640339Z","shell.execute_reply":"2025-09-28T08:48:45.871269Z"}},"outputs":[{"name":"stdout","text":"SVM Accuracy: 1.0\nRandom Forest Accuracy: 0.9902912621359223\n","output_type":"stream"}],"execution_count":25},{"id":"83e286c1-d46d-46f7-a4ee-94f17659da9f","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"afd8a91a-3650-4bc4-8618-287d8a6fb662","cell_type":"code","source":"import joblib\njoblib.dump(svm_model, \"/kaggle/working/svm_model.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T09:50:15.735245Z","iopub.execute_input":"2025-09-28T09:50:15.735517Z","iopub.status.idle":"2025-09-28T09:50:15.742364Z","shell.execute_reply.started":"2025-09-28T09:50:15.735495Z","shell.execute_reply":"2025-09-28T09:50:15.741667Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/svm_model.pkl']"},"metadata":{}}],"execution_count":29},{"id":"18f4e951-a3d0-4e08-b5c4-1d7f097c1230","cell_type":"code","source":"# Save only weights\ntorch.save(resnet18_model.state_dict(), \"/kaggle/working/resnet18_weights.pth\")\n\n# OR save the entire model (bigger file, includes architecture)\ntorch.save(resnet18_model, \"/kaggle/working/resnet18_full.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T09:50:23.785220Z","iopub.execute_input":"2025-09-28T09:50:23.785906Z","iopub.status.idle":"2025-09-28T09:50:24.005633Z","shell.execute_reply.started":"2025-09-28T09:50:23.785878Z","shell.execute_reply":"2025-09-28T09:50:24.005028Z"}},"outputs":[],"execution_count":30}]}